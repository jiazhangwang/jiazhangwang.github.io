<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Your Name - Portfolio</title>
    <style>
        body {
            font-family: 'Times New Roman', Times, serif, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
            color: #333;
            line-height: 1.6;
        }
        header {
            background-color: #333;
            color: #fff;
            padding: 20px 0;
            text-align: center;
        }
        header h1 {
            margin: 0;
            font-size: 2.5em;
        }
        header p {
            margin: 5px 0 0;
            font-size: 1.2em;
        }
        .container {
            max-width: 800px;
            margin: 20px auto;
            padding: 20px;
            background: #fff;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
        }
        h2 {
            color: #333;
            border-bottom: 2px solid #333;
            padding-bottom: 5px;
        }

        p3 {
            color: #333;
            font-size: 20px;
            font-weight: bold;
        }
        p4 {
            color: #333;
            font-size: 18px;
            font-weight: bold;
        }
        a {
            color: #0077cc;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        .section {
            margin-bottom: 30px;
        }
        .section ul {
            list-style-type: none;
            padding: 0;
        }
        .section ul li {
            margin-bottom: 10px;
        }
        .contact-info {
            text-align: center;
            margin-top: 20px;
        }
        .contact-info a {
            margin: 0 10px;
        }
        footer {
            text-align: center;
            padding: 10px 0;
            background-color: #333;
            color: #fff;
            margin-top: 20px;
        }
    </style>
</head>
<body>
    <header>
        <h1>Jiazhang Wang</h1>
        <p>Postdoctoral Researcher</p>
        <p>Wyant College of Optical Sciences, University of Arizona</p>
    </header>

    <div class="container">
        <div class="section">
            <h2>About Me</h2>
            <p>
                Hello! I'm a postdoctoral researcher at the <a href="https://3dim.optics.arizona.edu/" target="_blank">Computational 3D Imaging and Measurement Lab</a> , at Wyant College of Optical Sciences, University of Arizona. I earned my PH.D degree in Electrical Engineering from Northwestern University working with Prof. Florian Willomitzer and Prof. Oliver Cossairt. Prior to that, I received my M.S from Columbia University in 2019, and B.S from Huazhong University of Science and Technology in 2017. My research interests lie at the intersection of optical metrology, machine vision, and computational photography. I aim to develop the next generation of 3D imaging sensors by bridging the gap between high-precision optical metrology techniques and the widespread applicability of computer vision methods.
            </p>
        </div>

        <div class="contact-info">
            <p>
                <a href="mailto: jiazhangwang@arizona.edu">Email</a> |
                <a href="https://jiazhangwang.github.io/docs/resume.pdf">CV</a> |
                <a href="https://scholar.google.com/citations?user=kVHK8m8AAAAJ&hl=en">Publications</a> |
                <a href="https://www.linkedin.com/feed/">LinkedIn</a> |
                
            </p>
        </div>

        <div class="section">
            <h2>Featured Projects</h2>
            <ul>
                <p3>
                    Exploiting deflectometric information for high precision eye tracking: <br>
                </p3>
                <li>
                    </strong> Eye-tracking plays a crucial role in the development of virtual reality devices, neuroscience research, and psychology. However, an accurate, robust, and fast eye-tracking solution remains a considerable challenge for state-of-the-art methods. In our work, we rethink the way specular reflections on the eye surface can be used for fast and accurate eye tracking. Our approach introduces a novel method for eye tracking that combines principles from optical metrology and computational imaging. Compared to the state-of-the-art reflection-based eye tracking approaches, we easily improve the data density by a factor of >3300X, making our method more robust and accurate. We have levergaed different techniques to explore the deflectometic information to estimate the gaze direction, including stereo deflectometry, differentiable rendering and deep learning.
                </li>

                <img src="docs/eye_tracking.png" alt="Eye Tracking Project" class="project-image" style="  max-height: 300px; width: 100%; object-fit: contain; display: block; margin-top: 15px; ">
                <li>
                <p4>Associated Publications:</p4><br>
                    <a href="https://www.nature.com/articles/s41467-025-56801-1">Accurate Eye Tracking from Dense 3D Surface Reconstructions using Single-Shot Deflectometry</a> <br>
                    J. Wang, T. Wang, B. Xu, O. Cossairt, F. Willomitzer <br>
                    Nature Communications,2025 <br>

                    <a href="https://opg.optica.org/abstract.cfm?uri=FiO-2024-FTu6B.3">Deflectometric Eye-tracking on Human Eyes</a> <br>
                    J. Wang, T. Wang, B. Xu, O. Cossairt, F. Willomitzer <br>
                    OSA Frontiers in Optics + Laser Science (Emil Wolf Outstanding Student Paper Finalist),2024 <br>

                    <a href="https://ieeexplore.ieee.org/abstract/document/10491405">Differentiable Deflectometric Eye Tracking
                    </a> <br>
                    T. Wang, J. Wang, O. Cossairt, F. Willomitzer <br>
                    IEEE Transactions on Computational Imaging, 2024 <br>

                    <a href="https://preprints.opticaopen.org/articles/preprint/Accurate_Eye-Tracking_from_Deflectometric_Information_using_Deep_Learning/25199411/1/files/44494691.pdf">Accurate Eye-Tracking from Deflectometric Information using Deep Learning
                    </a> <br>
                    J. Choi, J. Wang, T. Wang, F. Willomitzer <br>
                    14th International Conference on Optics-Photonics Design and Fabrication (Best Student Paper), 2024 <br><br>
                </li>

                <p3>
                    Accurate 3D reconstruction of shiny and specular surfaces:<br>
                </p3>
                <li>
                </strong> Accurate and fast 3D imaging of specular or shiny surfaces still poses major challenges for state-of-the-art optical mea- surement principles. Most of common 3D imaging methods can only measure matt surface, such as active triangulation and time-of-flight. Frequently used methods, such as phase-measuring deflectometry (PMD) or shape-from-polarization (SfP), rely on strong assumptions about the measured objects, limiting their generalizability in broader application areas like medical imaging, industrial inspection, virtual reality, or cultural heritage analysis. In our group, we tackle this issue by leveraging and fusing different 3D imaging modalities. For example, We creatively combine polarization cues from SfP with geometric information obtained from PMD to resolve all arising ambiguities and unrealistic assumptions in the 3D measurement. Moreover, we take the advantage of single-shot fringe triangulation and polarization to measure the shiny part for industrial inspection.
                </li>
                <img src="docs/pol.png" alt="pol Project" class="project-image" style="  max-height: 200px; width: 100%; object-fit: contain; display: block; margin-top: 15px; ">
                <p4>Associated Publications:</p4><br>
                <a href="https://doi.org/10.1364/OPTICA.538331">3D Imaging of Complex Specular Surfaces by Fusing Polarimetric and Deflectometric Information</a> <br>
                J. Wang, O. Cossairt, F. Willomitzer <br>
                OPTICA,2025 <br>

                <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13135/1313507/Polarization-guided-deflectometry/10.1117/12.3027356.full">Polarization-guided deflectometry</a> <br>
                J. Wang, O. Cossairt, F. Willomitzer <br>
                SPIE Optics and Photonics,Interferometry and Structured Light,2024 <br>

                <a href="https://opg.optica.org/abstract.cfm?uri=FiO-2023-FTu6D.3">“On-the-Fly” Radiometric Calibration of Active 3D Imaging Setups using Superellipse Fitting</a> <br>
                J. Taylor, J. Wang, F. Willomitzer <br>
                OSA Frontiers in Optics + Laser,2023 <br><br>

                <p3>
                    Shape Measurement of Mixed Reflectance Scenes using Event-Triangulation-Deflectometry:<br>
                </p3>
                <li>
                </strong> Traditional optical 3D imaging techniques are limited to specific surface types, such as either diffuse or specular. In this research, we develop a novel technique that enables fast event-based 3D imaging of “mixed reflectance scenes” with high accuracy by jointly exploiting 3D imaging principles: active triangulation and deflectometry. On the captured events, we use epipolar constraints to separate the measured reflections into diffuse, two-bounce specular, and other multi-bounce reflections. The diffuse surfaces in the scene are reconstructed using event-triangulation. Then, the reconstructed diffuse scene is leveraged as a large virtual "display" to evaluate the specular scene via a novel event Deflectometry approach. Our proposed concept integrates active 3D imaging with neuromorphic sensor, leveraging its advantages—such as low latency and high dynamic range—to develop a versatile, "one fits all" 3D imaging solution. The technique significantly advances the state-of-the-art and our fundamental understanding of limits.
                </li>
                <img src="docs/event.png" alt="event" class="project-image" style="  max-height: 200px; width: 100%; object-fit: contain; display: block; margin-top: 15px; ">
                <p4>Associated Publications:</p4><br>
                <a href="https://arxiv.org/abs/2311.09652">Event-based Motion-Robust Shape Estimation for Mixed Reflectance Scenes</a> <br>
                A. Dashpute, J. Wang, J. Taylor, O. Cossairt, A. Veeraraghavan, and F. Willomitzer <br>
                ArXiv Preprint, 2024 <br>

                

            </ul>
        </div>



    </div>

    <footer>
        &copy; 2025 Jiazhang Wang. All rights reserved.
    </footer>
</body>
</html>